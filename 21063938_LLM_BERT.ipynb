{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWLnYRo_pCF",
        "outputId": "a21e9470-6c1d-4290-f78c-9765a088abf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The movie was really good. | Predicted Sentiment: Positive\n",
            "\n",
            "Sentence: The acting was terrible. | Predicted Sentiment: Negative\n",
            "\n",
            "Sentence: I loved every minute of it. | Predicted Sentiment: Negative\n",
            "\n",
            "Sentence: It was a waste of time. | Predicted Sentiment: Negative\n",
            "\n",
            "Sentence: The plot was confusing. | Predicted Sentiment: Negative\n",
            "\n",
            "Sentence: The cinematography was stunning. | Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load IMDB dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')  #Path to IMDB dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle dataset\n",
        "\n",
        "# Select 80 reviews\n",
        "reviews = df[\"review\"][:80].tolist()\n",
        "labels = df[\"sentiment\"][:80].tolist()\n",
        "\n",
        "# Map sentiment labels to numerical values\n",
        "label_map = {\"negative\": 0, \"positive\": 1}\n",
        "labels = [label_map[label] for label in labels]\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define dataset class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, tokenizer, max_length):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = self.reviews[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(review,\n",
        "                                  max_length=self.max_length,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_tensors='pt')\n",
        "        return {'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)}\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Prepare data loaders\n",
        "train_dataset = IMDBDataset(train_reviews, train_labels, tokenizer, max_length=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "# Evaluation\n",
        "test_dataset = IMDBDataset(test_reviews, test_labels, tokenizer, max_length=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probabilities = nn.functional.softmax(logits, dim=1)\n",
        "        predicted_labels = np.argmax(probabilities.cpu().numpy(), axis=1)\n",
        "\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Deployment\n",
        "# Save the trained model\n",
        "model_save_path = \"bert_sentiment_model.pt\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "# Load the saved model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Define a function for inference\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    inputs.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = nn.functional.softmax(logits, dim=1)\n",
        "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "    return predicted_label\n",
        "\n",
        "# Inference on new data\n",
        "new_sentences = [\"The movie was really good.\", \"The acting was terrible.\",\n",
        "                 \"I loved every minute of it.\", \"It was a waste of time.\",\n",
        "                 \"The plot was confusing.\", \"The cinematography was stunning.\"]\n",
        "new_labels = [1, 0, 1, 0, 0, 1]  # Assuming 0 represents negative and 1 represents positive\n",
        "\n",
        "for sent, label in zip(new_sentences, new_labels):\n",
        "    sentiment = \"Positive\" if predict_sentiment(sent) == 1 else \"Negative\"\n",
        "    print(f\"Sentence: {sent} | Predicted Sentiment: {sentiment}\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}